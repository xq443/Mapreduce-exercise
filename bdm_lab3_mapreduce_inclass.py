# -*- coding: utf-8 -*-
"""BDM_Lab3_MapReduce_InClass

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/189mHX4So2MM0g_vPULH02oyGMNIUjgUS

# Lab 3 - MapReduce

In this lab, we practice the MapReduce programming paradigm.

We will complete the tasks using the accompanied *mapreduce* package (as **mapreduce.py**) and MRJob. Please download the **mapreduce.py** file from our online class resource page, and place it in the same folder with your notebook.

For each invocation of an MapReduce job (with mr.run()), you are expected to supply a mapper, a reducer and/or a combiner as needed. Below are sample usage of the package:

```python
    # Run on input1 using your mapper1 and reducer1 function
    output = list(mr.run(input1, mapper1, reducer1))

    # Run on input2 using only your mapper2, no reduce phase
    output = list(mr.run(enumerate(input2), mapper2, combiner2))
    
    # Run on input3 using 2 nested MapReduce jobs
    output = mr.run(mr.run(input3, mapper3, reducer3), mapper4)
```
    
Please note that the input must be an iteratable of **key/value pairs**. If your inpu tdata does not have a key, you can simply add a null or index key through **enumerator(input)**. The output of the mr.run() is always a **generator**. You have to cast it to a list if you'd like to view, index or print it out.

We will also need **book.txt** and **citibike.csv** to be downloaded.
"""

!pip install mrjob

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

drive.CreateFile({'id': '1sq4-zXn2Z82mdLSBBegEgsUsfqtgza-C'}).GetContentFile('mapreduce.py')
drive.CreateFile({'id': '1qCQ6edyhTA1kqFWZf1y65ogidivDbBIT'}).GetContentFile('book.txt')
drive.CreateFile({'id': '1I8eqA1Zy3vFq4mN8z0ZRl7ABXrdzCRYI'}).GetContentFile('citibike.csv')

import csv
import mapreduce as mr

"""## Task 0

Here is another concrete example on "Word Count" using the package. Assuming we have a text file named *book.txt*. Our task is to count the frequency of words in this document, and print the top 10. For illustration purposes, we use only the first 1000 lines of the book for counting.
"""

with open('book.txt', 'r') as fi:
    lines = [(i,line.strip()) for i,line in enumerate(fi) if i<1000]

### After this, 'lines' stores a list of 1000 text lines
def mapper(k1, line):
    for word in line.strip().split(' '):
        if len(word)>0:
            yield (word, 1)
    
def reducer(word, counts):
    yield (word, sum(counts))

wCounts = list(mr.run(lines, mapper, reducer))
sortedCounts = sorted(wCounts, key=lambda x: -x[1])
sortedCounts[:10]

!head -n 2 citibike.csv

import csv

list(enumerate(next(open('citibike.csv', 'r')).strip().split(',')))

"""## Task 1

We would like to write a MapReduce job to count the total number of trips involved at each station. For example, if a trip starts at station A and stops at station B, the trip will count for both A and B. The output must be tuples, each consisting of a station name and a count.
"""

def mapper1(_, line):
  row = line.split(',')
  yield (row[6],1)
  yield (row[10],1)

def reducer1(station, counts):
  yield (station, sum(counts))
    
with open('citibike.csv', 'r') as fi:
    reader = enumerate(fi)
    output1 = list(mr.run(reader, mapper1, reducer1))

output1[:10]

"""
## Task 2

Below is an example of showing how to use nested jobs and jobs with mappers only using the mapreduce package, thus, no points are included. Our task here is that we would like to filter the output of Task 1 to display only those stations with more than 1000 trips involved, of course, using the MapReduce paradigm."""

def mapper2(station, count):
    if count>1000:
      yield (station, count)

with open('citibike.csv', 'r') as fi:
    reader = enumerate(csv.DictReader(fi))
    output2 = list(mr.run(mr.run(reader, mapper1, reducer1), mapper2))

output2

"""
## Task 3

We would like to count the number of trips taken between pairs of stations. Trips taken from station A to station B or  from station B to station A are both counted towards the station pair A and B. Please note that the station pair shoud be identified by station names, as a tuple, and in lexical order, i.e. (A,B) instead of (B,A) in this case. The output must be tuples, each consisting of the station pair identification and a count."""

tuple(sorted(('B', 'A')))

def mapper3(_, row):
  #yield (sorted([row['start_station_name'],row['end_station_name']]),1)
  if row['start_station_name']<row['end_station_name']:
    yield ((row['start_station_name'],row['end_station_name']),1)
  else: 
    yield ((row['end_station_name'],row['start_station_name']),1)

def reducer3(station_pair, counts):
  yield (tuple(station_pair), sum(counts))

with open('citibike.csv', 'r') as fi:
    reader = enumerate(csv.DictReader(fi))
    output3 = list(mr.run(reader, mapper3, reducer3))

output3[:10]

"""
## Task 4

In this task, you are asked to compute the station with the most riders started from, per each gender of the *'Subscriber'* user. Meaning, what was the station name with the highest number of bike pickups for female riders, for male riders and for unknown riders.

The output will be a list of tuples, each includes a gender label (as indicated below) and another tuple consisting of a station name, and the total number of trips started at that station for that gender.


The label mapping for the gender column in citibike.csv is: (Zero=<b>Unknown</b>; 1=<b>Male</b>; 2=<b>Female</b>)"""

def mapper4(_, row):
  if row['usertype'] == 'Subscriber':
    yield ((row['gender'], row['start_station_name']),1)

def reducer4(gender_station, counts):
  gender, station = gender_station
  yield (gender, (station, sum(counts)))

def mapper5(gender, station_counts):
  if gender=='0':
    gender = 'Unknown'
  elif gender=='1':
    gender = 'Male'
  else:
    gender = 'Female'
  yield (gender, station_counts)

def reducer5(gender, list_of_station_counts):
  yield (gender, max(list_of_station_counts, key=lambda x: x[1]))

with open('citibike.csv', 'r') as fi:
    reader = enumerate(csv.DictReader(fi))
    output5 = list(mr.run(mr.run(reader, mapper4, reducer4), mapper5, reducer5))

output5[:10]

d = [('Z St & Jay St', 25),
     ('York St & Jay St', 81),
     ('Stanton St & Mangin St', 1)]
max(d, key=lambda x:x[1])

"""## Task 5

MRJob is a convenient packages for simplifying the execution of MapReduce jobs on clusters. However, it doesn't work in a notebook. We're going to convert some of the examples of MRJob into our notebooks so that we can test our code before deploying them on Hadoop.

The two examples are available at:

https://mrjob.readthedocs.io/en/latest/guides/quickstart.html

https://mrjob.readthedocs.io/en/latest/guides/writing-mrjobs.html
"""

from mrjob.job import MRJob

class MRWordFrequencyCount(MRJob):

    def mapper(self, _, line):
        yield "chars", len(line)
        yield "words", len(line.split())
        yield "lines", 1

    def reducer(self, key, values):
        yield key, sum(values)

from mrjob.job import MRJob

class MRStationCount(MRJob):

  def mapper(self, _, line):
    row = line.split(',')
    yield (row[6],1)
    yield (row[10],1)

  def reducer(self, station, counts):
    yield (station, sum(counts))
  
stationCount = MRStationCount(args=[])
counts = list(mr.runJob(enumerate(open('citibike.csv', 'r')), stationCount))
counts[:10]

with open('book.txt', 'r') as fi:
    lines = [(i,line.strip()) for i,line in enumerate(fi) if i<1000]

wordcount = MRWordFrequencyCount(args=[])
wCounts = mr.runJob(lines, wordcount)
sortedCounts = sorted(wCounts, key=lambda x: -x[1])
sortedCounts[:10]

"""## Task 6

Let's try to run the above MRJob examples as stand-alone applications. Please check again:
https://mrjob.readthedocs.io/en/latest/guides/writing-mrjobs.html#defining-steps
"""

from mrjob.step import MRStep
import re

WORD_RE = re.compile(r"[\w']+")


class MRMostUsedWord(MRJob):

    def steps(self):
        return [
            MRStep(mapper=self.mapper_get_words,
                   combiner=self.combiner_count_words,
                   reducer=self.reducer_count_words),
            MRStep(reducer=self.reducer_find_max_word)
        ]

    def mapper_get_words(self, _, line):
        # yield each word in the line
        for word in WORD_RE.findall(line):
            yield (word.lower(), 1)

    def combiner_count_words(self, word, counts):
        # optimization: sum the words we've seen so far
        yield (word, sum(counts))

    def reducer_count_words(self, word, counts):
        # send all (num_occurrences, word) pairs to the same reducer.
        # num_occurrences is so we can easily use Python's max() function.
        yield None, (sum(counts), word)

    # discard the key; it is just None
    def reducer_find_max_word(self, _, word_count_pairs):
        # each item of word_count_pairs is (count, word),
        # so yielding one results in key=counts, value=word
        yield max(word_count_pairs)

mostused = MRMostUsedWord(args=[]) 
wCounts = mr.runJob(lines, mostused)
list(wCounts)

